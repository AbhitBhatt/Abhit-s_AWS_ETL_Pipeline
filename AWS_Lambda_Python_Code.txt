import json
import boto3
import pandas as pd
import io
from datetime import datetime


# ----------------------------------------
# ðŸ”„ Flatten JSON: Convert nested structure to tabular using pandas
# ----------------------------------------
def flatten(data):
    orders_data=[]
    for order in data:
        for product in order['products']:  
                    row_orders = {
                        "order_id" : order["order_id"],
                        "order_date" : order["order_date"],
                        "total_amount" : order["total_amount"],
                        "customer_id" : order["customer"]["customer_id"],
                        "customer_name" : order["customer"]["name"],
                        "email" : order["customer"]["email"],
                        "address" : order["customer"]["address"],
                        "product_id" : product["product_id"],
                        "product_name" : product["name"],
                        "category" : product["category"],
                        "price" : product["price"],
                        "quantity" : product["quantity"]
                    } 
                    orders_data.append(row_orders)   
    df_orders = pd.DataFrame(orders_data)
    return df_orders

# ----------------------------------------
# ðŸš€ Lambda Handler: Triggered when a new file is uploaded to S3
# ----------------------------------------

def lambda_handler(event, context):

# ðŸ“¥ Read JSON file from S3   

    bucket_name=event['Records'][0]['s3']['bucket']['name']
    file_name=event['Records'][0]['s3']['object']['key']

    s3=boto3.client('s3')
    response=s3.get_object(Bucket=bucket_name,Key=file_name)

# Parse the JSON content

    content=response['Body'].read().decode('utf-8')
    data=json.loads(content)

# ----------------------------------------
# ðŸ§¾ Flatten JSON to tabular format
# ----------------------------------------

    df=flatten(data)
    
# ðŸ“¦ Convert DataFrame to Parquet format (in-memory)

    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, index=False, engine='pyarrow')

# Generate unique file name using timestamp

    now = datetime.now()
    timestamp = now.strftime("%Y%m%d_%H%M%S")

    key_staging=f'ordders_parquet/orders_ETL_{timestamp}.parquet'
    
# ----------------------------------------
# ðŸ’¾ Upload Parquet file to target S3 folder
# ----------------------------------------

s3.put_object(Bucket=bucket_name, Key=key_staging
                    , Body=parquet_buffer.getvalue())

# ----------------------------------------
# ðŸ§  Start AWS Glue Crawler to update metadata
# ----------------------------------------

    crawler_name='abhit_crawler1'
    glue=boto3.client('glue')
    response=glue.start_crawler(Name=crawler_name)

# âœ… Lambda response
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
